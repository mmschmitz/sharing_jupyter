{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 Cities Data Analysis - basic code\n",
    "# Written by Michelle Schmitz, originally in Jupyter notebooks and then in Python code.\n",
    "# Initially written on 02 March 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing in pandas and numpy libraries to handle data management aspects of the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd \n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bfe2c9d15306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'https://data.cdc.gov/api/views/6vp6-wxuq/rows.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdata_500_cities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdata_500_cities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\n\u001b[1;32m--> 424\u001b[1;33m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[0;32m    425\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compression'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gzip'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readall_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readall_chunked\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m                 \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1050\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1052\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Reading in the 500 Cities data set, created by the CDC's Division of Population Health (2018 release)\n",
    "\n",
    "## This is the original dataset's link:\n",
    "# read.csv(\"https://catalog.data.gov/dataset/500-cities-local-data-for-better-health-b32fd/resource/8a49a1f7-4fcc-49a6-acb5-fcd3c0796782\")\n",
    "\n",
    "url = ('https://data.cdc.gov/api/views/6vp6-wxuq/rows.csv')\n",
    "\n",
    "data_500_cities = pd.read_csv(url)\n",
    "data_500_cities.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the first state as well\n",
    "data_500_cities[data_500_cities.StateAbbr == 'AL'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# doing more specific descriptive checks for all of the data - and the numeric data in particular\n",
    "data_500_cities.info() #basic information about the dataset\n",
    "data_500_cities.dtypes\n",
    "data_500_cities.describe() #descriptive statistics for all numeric variables - including FIPS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examining categorical values of Measures variable, creating series object\n",
    "Col_Measure = ['CityName','StateAbbr','DataValueTypeID','Measure','Data_Value','Low_Confidence_Limit','High_Confidence_Limit','CityFIPS','TractFIPS','PopulationCount','GeoLocation','UniqueID'] \n",
    "print(Col_Measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for Age-Adjusted Prevalence (updated)\n",
    "pivot1 = pd.pivot_table(data_500_cities[Col_Measure][(data_500_cities.DataValueTypeID == 'AgeAdjPrv') & (data_500_cities.StateAbbr == 'US')], values=['Low_Confidence_Limit','Data_Value','High_Confidence_Limit'], index='Measure', margins=False, dropna=False, aggfunc={'Low_Confidence_Limit': np.mean, 'Data_Value': np.mean,'High_Confidence_Limit': np.mean})\n",
    "pivot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pivot for Crude Prevalence (updated)\n",
    "pivot2 = pd.pivot_table(data_500_cities[Col_Measure][(data_500_cities.DataValueTypeID == 'CrdPrv') & (data_500_cities.StateAbbr == 'US')], values=['Low_Confidence_Limit','Data_Value','High_Confidence_Limit'], index='Measure', margins=False, dropna=False, aggfunc={'Low_Confidence_Limit': np.mean, 'Data_Value': np.mean,'High_Confidence_Limit': np.mean})\n",
    "pivot2\n",
    "#print(pivot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to display the data for US overall, and averaged for all counties together\n",
    "\n",
    "## pivot 1\n",
    "\n",
    "#converting back into another data frame to display data together\n",
    "flattened1 = pd.DataFrame(pivot1.to_records())\n",
    "with_new_index1 = flattened1.set_index('Measure')\n",
    "with_new_index1.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.AgeAdjPrv').replace('High_Confidence_Limit','High_CL.AgeAdjPrv').replace('Data_Value','Data_Value.AgeAdjPrv') \\\n",
    "                           for hdr in with_new_index1.columns]\n",
    "with_new_index1\n",
    "\n",
    "## pivot 2\n",
    "\n",
    "#converting back into another data frame to display data together\n",
    "flattened2 = pd.DataFrame(pivot2.to_records())\n",
    "with_new_index2 = flattened2.set_index('Measure')\n",
    "with_new_index2.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.CrdPrv').replace('High_Confidence_Limit','High_CL.CrdPrv').replace('Data_Value','Data_Value.CrdPrv') \\\n",
    "                           for hdr in with_new_index2.columns]\n",
    "\n",
    "with_new_index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining 2 pivot tables together in case if I want to output larger table\n",
    "# Dropping a good reference on different types of joins here -- http://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/\n",
    "\n",
    "US_data_table = pd.merge(with_new_index1, with_new_index2, on='Measure', how='outer')\n",
    "US_data_table\n",
    "US_data_table_ft = pd.DataFrame(US_data_table.to_records())\n",
    "US_data_table_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reordering columns so that it makes sense in a comparison framework\n",
    "US_data_table_index = US_data_table_ft.set_index('Measure')\n",
    "US_data_table_ro = US_data_table_index[['Low_CL.CrdPrv','Data_Value.CrdPrv','High_CL.CrdPrv','Low_CL.AgeAdjPrv','Data_Value.AgeAdjPrv','High_CL.AgeAdjPrv']]\n",
    "US_data_table_ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that I've presented the data in a nice-to-see way for the entire US - let's see what the distribution is of\n",
    "# the PREVALENCES (both CRUDE and AGE-ADJUSTED) for all cities (n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ALL PREVALENCES for all observations that ARE NOT the US summary (i.e., not summed across US)\n",
    "DF_Prev = data_500_cities[Col_Measure][(data_500_cities.StateAbbr != 'US')] \n",
    "DF_Prev\n",
    "\n",
    "## However, we need to know how many observations are in each grouping; age-adjusted prevalences are demographically adjusted based on age distributions from each crude population!\n",
    "DF_Prev.groupby('DataValueTypeID').groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will split the prevalences dataset into crude and age-adjusted prevalences, before splitting them further \n",
    "# into each measure and recombining them. This reshapes the dataset so each Measure will have its own column,\n",
    "# making it easier to compare across measures for future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recoding Measure into MeasureShort\n",
    "\n",
    "conditions = [\n",
    "    (DF_Prev['Measure'] == 'All teeth lost among adults aged >=65 Years'),\n",
    "    (DF_Prev['Measure'] == 'Arthritis among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Binge drinking among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Cancer (excluding skin cancer) among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Cholesterol screening among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Chronic kidney disease among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Chronic obstructive pulmonary disease among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Coronary heart disease among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Current asthma among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Current lack of health insurance among adults aged 18–64 Years'),\n",
    "    (DF_Prev['Measure'] == 'Current smoking among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Diagnosed diabetes among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50–75 Years'),\n",
    "    (DF_Prev['Measure'] == 'High blood pressure among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'High cholesterol among adults aged >=18 Years who have been screened in the past 5 Years'),\n",
    "    (DF_Prev['Measure'] == 'Mammography use among women aged 50–74 Years'),\n",
    "    (DF_Prev['Measure'] == 'Mental health not good for >=14 days among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'No leisure-time physical activity among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Obesity among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Older adult men aged >=65 Years who are up to date on a core set of clinical preventive services: Flu shot past Year, PPV shot ever, Colorectal cancer screening'),\n",
    "    (DF_Prev['Measure'] == 'Older adult women aged >=65 Years who are up to date on a core set of clinical preventive services: Flu shot past Year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 Years'),\n",
    "    (DF_Prev['Measure'] == 'Papanicolaou smear use among adult women aged 21–65 Years'),\n",
    "    (DF_Prev['Measure'] == 'Physical health not good for >=14 days among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Sleeping less than 7 hours among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Stroke among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Taking medicine for high blood pressure control among adults aged >=18 Years with high blood pressure'),\n",
    "    (DF_Prev['Measure'] == 'Visits to dentist or dental clinic among adults aged >=18 Years'),\n",
    "    (DF_Prev['Measure'] == 'Visits to doctor for routine checkup within the past Year among adults aged >=18 Years')\n",
    "     ]\n",
    "\n",
    "choices = [\n",
    "    'TeethLost', 'Arthritis', 'BngDrnk', 'Cancer', 'Cholesterol', 'KidneyDis', 'COPD', \n",
    "    'CHD', 'Asthma', 'NoHlthIns', 'CurrSmoke', 'Diabetes','FecBldTst', 'HighBP', 'HighChol',\n",
    "    'Mammo', 'MentHlth', 'NoPhysAct', 'Obesity', 'OlderMen', 'OlderWomen', 'PapSmear', \n",
    "    'PhysHlthBad', 'SleepLittle', 'Stroke', 'HtnMeds', 'DentalVisits', 'DocVisits'\n",
    "    ]\n",
    "\n",
    "DF_Prev['Measure_Short'] = np.select(conditions, choices)\n",
    "\n",
    "#print(DF_Prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tab to check the outputs of my shortened measure variable\n",
    "pd.crosstab(DF_Prev['Measure'],DF_Prev['Measure_Short'])\n",
    "\n",
    "DF_Prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the CRUDE PREVALENCE for all observations that ARE NOT the US summary\n",
    "DF_CrdPrev = DF_Prev[(DF_Prev.DataValueTypeID == 'CrdPrv')]\n",
    "DF_CrdPrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boxplot of crude prevalences\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Boxplot, Crude Prevalence')\n",
    "pd.DataFrame(DF_CrdPrev.Data_Value).boxplot(grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the AGE-ADJUSTED PREVALENCE for all observations that ARE NOT the US summary\n",
    "DF_AgeAdjPrev = DF_Prev[(DF_Prev.DataValueTypeID == 'AgeAdjPrv')]\n",
    "DF_AgeAdjPrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BOXPLOT OF AGE-ADJUSTED PREVALENCE (which is adjusted based on all of the crude prevalences) - \n",
    "# This is for all causes smashed together\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Boxplot, Age-Adjusted Prevalence')\n",
    "#pd.DataFrame(DF_AgeAdjPrev_ft.Data_Value).boxplot(grid=False)\n",
    "pd.DataFrame(DF_AgeAdjPrev.Data_Value).boxplot(grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Breaking up the CRUDE PREVALENCES dataset by each measure grouping\n",
    "measure_groups = DF_CrdPrev.groupby('Measure_Short').groups\n",
    "measure_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting keys of the dataset (so we can break the datasets up appropriately)\n",
    "gb = DF_CrdPrev.groupby('Measure_Short')\n",
    "gb.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I was better at Python, I'd basically do a FOR loop for the above set of Dict Keys, \n",
    "# to basically run the set of commands that would break up each dataset, rename some columns and read them\n",
    "# to the entire dataset. \n",
    "\n",
    "# However, I've only given myself a day to write up this analysis, and I'm out of practice.\n",
    "\n",
    "# I will revisit the FOR loop idea at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wanted to see the distributions of the 28 variables.\n",
    "# Could make paneled histograms, but boxplots and other measures could be interesting too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas boxplot\n",
    "DF_CrdPrev.boxplot(by='Measure_Short', \n",
    "                       column=['Data_Value'], \n",
    "                       grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn boxplot\n",
    "sns.boxplot(y='Data_Value', x='Measure_Short', \n",
    "                 data=DF_CrdPrev, \n",
    "                 width=0.5,\n",
    "                 palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boxplots weren't helpful, and the axes did not leave much to be seen.\n",
    "# # I decided to try to do a paneled histogram, to see if we could see approximate distributions.\n",
    "# # I was still interested in comparisons, though...\n",
    "\n",
    "# #link 1: https://realpython.com/python-histograms/\n",
    "# #link 2: https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html\n",
    "\n",
    "# #fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "# #ax0, ax1, ax2, ax3 = axes.flatten()\n",
    "\n",
    "# # Set up the plot\n",
    "# #ax = plt.subplot(14, 2, Measure_i) #set it up to be a very long, very narrow plot\n",
    "# # fig = plt.figure()\n",
    "# # fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "                    \n",
    "# for Measure_i in range(1, 28):\n",
    "\n",
    "# #     fig = plt.figure()\n",
    "# #     fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "#     # Subset to the Measure of interest\n",
    "#     subset = DF_CrdPrev[DF_CrdPrev['Measure_Short'] == Measure_i]\n",
    "\n",
    "#     fig, ax = plt.subplots(14, 2, sharex='col', sharey='row')\n",
    "\n",
    "#     # Draw the plot\n",
    "#     ax.hist(subset['Data_Value'], bins = 10,\n",
    "#              color = 'blue', edgecolor = 'black', label = Measure_i)\n",
    "    \n",
    "#     # Title and labels\n",
    "# #    ax.set_title('Histogram', size = 10)\n",
    "#     ax.set_xlabel('Prevalence (per 100,000)', size = 10)\n",
    "#     ax.set_ylabel('Flights', size= 10)\n",
    "\n",
    "# #plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the list of keys - as we have 28 variables, we can't use all variables in one graphic.\n",
    "list(gb.groups.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplots aren't helpful (surprise - there's 28 variables, with a lot of overlap!)\n",
    "# So, I'm creating a density plot of all of our measures to see what the relative densities are.\n",
    "\n",
    "# Remember: PREVALENCE is a measure of the population (new and current cases, per 100,000)\n",
    "# Density here measures the number of CENSUS TRACTS that have a certain prevalence.\n",
    "\n",
    "# idea taken from https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of first 7 indicators to plot\n",
    "Measures_Plot1 = ['Arthritis','Asthma','BngDrnk','CHD','COPD','Cancer','Cholesterol']\n",
    "\n",
    "# Iterate through the indicators\n",
    "for Measure_i in Measures_Plot1:\n",
    "    subset = DF_CrdPrev[DF_CrdPrev['Measure_Short'] == Measure_i]  # subset to measure of interest\n",
    " \n",
    "    # Draw the density plot\n",
    "    sns.distplot(subset['Data_Value'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = Measure_i)\n",
    "    \n",
    "plt.legend(prop={'size': 12}, title = 'Measure',bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Density Plot with Chronic Disease Measures (1)')\n",
    "plt.xlabel('Prevalence (per 100,000)')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of second 7 indicators to plot\n",
    "Measures_Plot2 = ['CurrSmoke','DentalVisits','Diabetes','DocVisits','FecBldTst','HighBP','HighChol']\n",
    "\n",
    "# Iterate through the indicators\n",
    "for Measure_i in Measures_Plot2:\n",
    "    # Subset to the Measure of interest\n",
    "    subset = DF_CrdPrev[DF_CrdPrev['Measure_Short'] == Measure_i]\n",
    "    \n",
    "    # Draw the density plot\n",
    "    sns.distplot(subset['Data_Value'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = Measure_i)\n",
    "    \n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12}, title = 'Measure',bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Density Plot with Chronic Disease Measures (2)')\n",
    "plt.xlabel('Prevalence (per 100,000)')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of third 7 indicators to plot\n",
    "Measures_Plot3 = ['HtnMeds','KidneyDis','Mammo','MentHlth','NoHlthIns','NoPhysAct','Obesity']\n",
    "\n",
    "# Iterate through the indicators\n",
    "for Measure_i in Measures_Plot3:\n",
    "    # Subset to the Measure of interest\n",
    "    subset = DF_CrdPrev[DF_CrdPrev['Measure_Short'] == Measure_i]\n",
    "    \n",
    "    # Draw the density plot\n",
    "    sns.distplot(subset['Data_Value'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = Measure_i)\n",
    "    \n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12}, title = 'Measure',bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Density Plot with Chronic Disease Measures (3)')\n",
    "plt.xlabel('Prevalence (per 100,000)')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of last 7 indicators to plot\n",
    "Measures_Plot4 = ['OlderMen','OlderWomen','PapSmear','PhysHlthBad','SleepLittle','Stroke','TeethLost']\n",
    "\n",
    "# Iterate through the indicators\n",
    "for Measure_i in Measures_Plot4:\n",
    "    # Subset to the Measure of interest\n",
    "    subset = DF_CrdPrev[DF_CrdPrev['Measure_Short'] == Measure_i]\n",
    "    \n",
    "    # Draw the density plot\n",
    "    sns.distplot(subset['Data_Value'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = Measure_i)\n",
    "    \n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12}, title = 'Measure',bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Density Plot with Chronic Disease Measures (4)')\n",
    "plt.xlabel('Prevalence (per 100,000)')\n",
    "plt.ylabel('Density')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT DOES THE ABOVE ANALYSIS TELL US?\n",
    "\n",
    "A lot of census tracts are reporting a low prevalence of many chronic conditions, such as CHD, stroke, and diabetes (i.e., high density at a very low prevalence).\n",
    "\n",
    "Obviously, the indicators with much higher prevalences are those that affect larger swathes of the population such as mammograms and Pap smears (for women), dental visits and doctor's visits (ideally for the entire population).\n",
    "\n",
    " However, there were some outcomes I was intrigued in - sleeping little, Pap smears, hypertension meds, and cholesterol that I want to map out - because the density plot can't tell us much. We know there are spatial relationships to some of these indicators - especially here in the south, where there's worse health outcomes overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My interests, for a long time, have been in spatial epidemiology - the effects of place upon health outcomes.\n",
    "\n",
    "This dataset, as part of the 500 Cities project, would play well with this interest.\n",
    "\n",
    "So, I decided to reshape my dataset, so each indicator's census tract was a distinct row and each measure had its own column. I figured this would play better with the mapping capabilities found in Geopandas, Pandas, and MatPlotLib\n",
    "\n",
    "Link:\n",
    "https://towardsdatascience.com/lets-make-a-map-using-geopandas-pandas-and-matplotlib-to-make-a-chloropleth-map-dddc31c1983d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is my code to reshape a dataset so I had more control over the columns - \n",
    "# say, if I was creating a dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into seperate instances\n",
    "Arthritis = gb.get_group('Arthritis')\n",
    "Arthritis.drop(columns=['Measure']) #Measure used to be specific to each measure - we're outputting data, so not necessary\n",
    "Arthritis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the index variable and starting to rename variables.\n",
    "Arthritis_index = Arthritis.set_index('UniqueID')\n",
    "Arthritis_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.Arthritis').replace('High_Confidence_Limit','High_CL.Arthritis').replace('Data_Value','Data_Value.Arthritis').replace('PopulationCount','PopCount') \\\n",
    "                           for hdr in Arthritis_index.columns]\n",
    "Arthritis_index.head()\n",
    "Arthritis = Arthritis_index\n",
    "\n",
    "#check to make sure I removed Arthritis index correctly\n",
    "#del Arthritis_index\n",
    "Arthritis\n",
    "Arthritis_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All other indicators will only have Unique Identifiers and the CLs and Data Values Attached to Them\n",
    "\n",
    "Col_Grps = ['Low_Confidence_Limit','High_Confidence_Limit','Data_Value','UniqueID'] \n",
    "print(Col_Grps)\n",
    "\n",
    "#List: 'Asthma', 'BngDrnk', 'CHD', 'COPD', 'Cancer', 'Cholesterol', 'CurrSmoke', 'DentalVisits', 'Diabetes', 'DocVisits', 'FecBldTst', 'HighBP', 'HighChol', 'HtnMeds', 'KidneyDis', 'Mammo', 'MentHlth', 'NoHlthIns', 'NoPhysAct', 'Obesity', 'OlderMen', 'OlderWomen', 'PapSmear', 'PhysHlthBad', 'SleepLittle', 'Stroke', 'TeethLost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asthma\n",
    "Asthma = gb.get_group('Asthma')[Col_Grps]\n",
    "Asthma.head()\n",
    "Asthma_index = Asthma.set_index('UniqueID')\n",
    "Asthma_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.Asthma').replace('High_Confidence_Limit','High_CL.Asthma').replace('Data_Value','Data_Value.Asthma') \\\n",
    "                           for hdr in Asthma_index.columns]\n",
    "Asthma_index.head()\n",
    "#Asthma = Asthma_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Binge Drinking\n",
    "BngDrnk = gb.get_group('BngDrnk')[Col_Grps]\n",
    "#BngDrnk.head()\n",
    "BngDrnk_index = BngDrnk.set_index('UniqueID')\n",
    "BngDrnk_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.BngDrnk').replace('High_Confidence_Limit','High_CL.BngDrnk').replace('Data_Value','Data_Value.BngDrnk') \\\n",
    "                           for hdr in BngDrnk_index.columns]\n",
    "BngDrnk_index.head()\n",
    "#BngDrnk = BngDrnk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHD\n",
    "CHD = gb.get_group('CHD')[Col_Grps]\n",
    "#CHD.head()\n",
    "CHD_index = CHD.set_index('UniqueID')\n",
    "CHD_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.CHD').replace('High_Confidence_Limit','High_CL.CHD').replace('Data_Value','Data_Value.CHD') \\\n",
    "                           for hdr in CHD_index.columns]\n",
    "CHD_index.head()\n",
    "#CHD = CHD_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COPD\n",
    "COPD = gb.get_group('COPD')[Col_Grps]\n",
    "#COPD.head()\n",
    "COPD_index = COPD.set_index('UniqueID')\n",
    "COPD_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.COPD').replace('High_Confidence_Limit','High_CL.COPD').replace('Data_Value','Data_Value.COPD') \\\n",
    "                           for hdr in COPD_index.columns]\n",
    "COPD_index.head()\n",
    "#COPD = COPD_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cancer\n",
    "Cancer = gb.get_group('Cancer')[Col_Grps]\n",
    "#Cancer.head()\n",
    "Cancer_index = Cancer.set_index('UniqueID')\n",
    "Cancer_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.Cancer').replace('High_Confidence_Limit','High_CL.Cancer').replace('Data_Value','Data_Value.Cancer') \\\n",
    "                           for hdr in Cancer_index.columns]\n",
    "Cancer_index.head()\n",
    "#Cancer = Cancer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cholesterol\n",
    "Cholesterol = gb.get_group('Cholesterol')[Col_Grps]\n",
    "#Cholesterol.head()\n",
    "Cholesterol_index = Cholesterol.set_index('UniqueID')\n",
    "Cholesterol_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.Cholesterol').replace('High_Confidence_Limit','High_CL.Cholesterol').replace('Data_Value','Data_Value.Cholesterol') \\\n",
    "                           for hdr in Cholesterol_index.columns]\n",
    "Cholesterol_index.head()\n",
    "#Cholesterol = Cholesterol_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current Smoking\n",
    "CurrSmoke = gb.get_group('CurrSmoke')[Col_Grps]\n",
    "#CurrSmoke.head()\n",
    "CurrSmoke_index = CurrSmoke.set_index('UniqueID')\n",
    "CurrSmoke_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.CurrSmoke').replace('High_Confidence_Limit','High_CL.CurrSmoke').replace('Data_Value','Data_Value.CurrSmoke') \\\n",
    "                           for hdr in CurrSmoke_index.columns]\n",
    "CurrSmoke_index.head()\n",
    "#CurrSmoke = CurrSmoke_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dental Visits\n",
    "DentalVisits = gb.get_group('DentalVisits')[Col_Grps]\n",
    "#DentalVisits.head()\n",
    "DentalVisits_index = DentalVisits.set_index('UniqueID')\n",
    "DentalVisits_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.DentalVisits').replace('High_Confidence_Limit','High_CL.DentalVisits').replace('Data_Value','Data_Value.DentalVisits') \\\n",
    "                           for hdr in DentalVisits_index.columns]\n",
    "DentalVisits_index.head()\n",
    "#DentalVisits = DentalVisits_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diabetes\n",
    "Diabetes = gb.get_group('Diabetes')[Col_Grps]\n",
    "#Diabetes.head()\n",
    "Diabetes_index = Diabetes.set_index('UniqueID')\n",
    "Diabetes_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.Diabetes').replace('High_Confidence_Limit','High_CL.Diabetes').replace('Data_Value','Data_Value.Diabetes') \\\n",
    "                           for hdr in Diabetes_index.columns]\n",
    "Diabetes_index.head()\n",
    "#Diabetes = Diabetes_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doctor's Visits\n",
    "DocVisits = gb.get_group('DocVisits')[Col_Grps]\n",
    "#DocVisits.head()\n",
    "DocVisits_index = DocVisits.set_index('UniqueID')\n",
    "DocVisits_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.DocVisits').replace('High_Confidence_Limit','High_CL.DocVisits').replace('Data_Value','Data_Value.DocVisits') \\\n",
    "                           for hdr in DocVisits_index.columns]\n",
    "DocVisits_index.head()\n",
    "#DocVisits = DocVisits_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fecal Blood Tests\n",
    "FecBldTst = gb.get_group('FecBldTst')[Col_Grps]\n",
    "#FecBldTst.head()\n",
    "FecBldTst_index = FecBldTst.set_index('UniqueID')\n",
    "FecBldTst_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.FecBldTst').replace('High_Confidence_Limit','High_CL.FecBldTst').replace('Data_Value','Data_Value.FecBldTst') \\\n",
    "                           for hdr in FecBldTst_index.columns]\n",
    "FecBldTst_index.head()\n",
    "#FecBldTst = FecBldTst_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Blood Pressures\n",
    "HighBP = gb.get_group('HighBP')[Col_Grps]\n",
    "#HighBP.head()\n",
    "HighBP_index = HighBP.set_index('UniqueID')\n",
    "HighBP_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.HighBP').replace('High_Confidence_Limit','High_CL.HighBP').replace('Data_Value','Data_Value.HighBP') \\\n",
    "                           for hdr in HighBP_index.columns]\n",
    "HighBP_index.head()\n",
    "#HighBP = HighBP_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Cholesterol\n",
    "HighChol = gb.get_group('HighChol')[Col_Grps]\n",
    "#HighChol.head()\n",
    "HighChol_index = HighChol.set_index('UniqueID')\n",
    "HighChol_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.HighChol').replace('High_Confidence_Limit','High_CL.HighChol').replace('Data_Value','Data_Value.HighChol') \\\n",
    "                           for hdr in HighChol_index.columns]\n",
    "HighChol_index.head()\n",
    "#HighChol = HighChol_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypertension Medication\n",
    "HtnMeds = gb.get_group('HtnMeds')[Col_Grps]\n",
    "#HtnMeds.head()\n",
    "HtnMeds_index = HtnMeds.set_index('UniqueID')\n",
    "HtnMeds_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.HtnMeds').replace('High_Confidence_Limit','High_CL.HtnMeds').replace('Data_Value','Data_Value.HtnMeds') \\\n",
    "                           for hdr in HtnMeds_index.columns]\n",
    "HtnMeds_index.head()\n",
    "#HtnMeds = HtnMeds_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kidney Disease\n",
    "KidneyDis = gb.get_group('KidneyDis')[Col_Grps]\n",
    "#KidneyDis.head()\n",
    "KidneyDis_index = KidneyDis.set_index('UniqueID')\n",
    "KidneyDis_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.KidneyDis').replace('High_Confidence_Limit','High_CL.KidneyDis').replace('Data_Value','Data_Value.KidneyDis') \\\n",
    "                           for hdr in KidneyDis_index.columns]\n",
    "KidneyDis_index.head()\n",
    "#KidneyDis = KidneyDis_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mammograms\n",
    "Mammo = gb.get_group('Mammo')[Col_Grps]\n",
    "#Mammo.head()\n",
    "Mammo_index = Mammo.set_index('UniqueID')\n",
    "Mammo_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.Mammo').replace('High_Confidence_Limit','High_CL.Mammo').replace('Data_Value','Data_Value.Mammo') \\\n",
    "                           for hdr in Mammo_index.columns]\n",
    "Mammo_index.head()\n",
    "#Mammo = Mammo_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mental Health\n",
    "MentHlth = gb.get_group('MentHlth')[Col_Grps]\n",
    "#MentHlth.head()\n",
    "MentHlth_index = MentHlth.set_index('UniqueID')\n",
    "MentHlth_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.MentHlth').replace('High_Confidence_Limit','High_CL.MentHlth').replace('Data_Value','Data_Value.MentHlth') \\\n",
    "                           for hdr in MentHlth_index.columns]\n",
    "MentHlth_index.head()\n",
    "#MentHlth = MentHlth_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Health Insurance\n",
    "NoHlthIns = gb.get_group('NoHlthIns')[Col_Grps]\n",
    "#NoHlthIns.head()\n",
    "NoHlthIns_index = NoHlthIns.set_index('UniqueID')\n",
    "NoHlthIns_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.NoHlthIns').replace('High_Confidence_Limit','High_CL.NoHlthIns').replace('Data_Value','Data_Value.NoHlthIns') \\\n",
    "                           for hdr in NoHlthIns_index.columns]\n",
    "NoHlthIns_index.head()\n",
    "#NoHlthIns = NoHlthIns_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Physical Activity\n",
    "NoPhysAct = gb.get_group('NoPhysAct')[Col_Grps]\n",
    "#NoPhysAct.head()\n",
    "NoPhysAct_index = NoPhysAct.set_index('UniqueID')\n",
    "NoPhysAct_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.NoPhysAct').replace('High_Confidence_Limit','High_CL.NoPhysAct').replace('Data_Value','Data_Value.NoPhysAct') \\\n",
    "                           for hdr in NoPhysAct_index.columns]\n",
    "NoPhysAct_index.head()\n",
    "#NoPhysAct = NoPhysAct_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obesity\n",
    "Obesity = gb.get_group('Obesity')[Col_Grps]\n",
    "#Obesity.head()\n",
    "Obesity_index = Obesity.set_index('UniqueID')\n",
    "Obesity_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.Obesity').replace('High_Confidence_Limit','High_CL.Obesity').replace('Data_Value','Data_Value.Obesity') \\\n",
    "                           for hdr in Obesity_index.columns]\n",
    "Obesity_index.head()\n",
    "#Obesity = Obesity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for Older Men\n",
    "OlderMen = gb.get_group('OlderMen')[Col_Grps]\n",
    "#OlderMen.head()\n",
    "OlderMen_index = OlderMen.set_index('UniqueID')\n",
    "OlderMen_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.OlderMen').replace('High_Confidence_Limit','High_CL.OlderMen').replace('Data_Value','Data_Value.OlderMen') \\\n",
    "                           for hdr in OlderMen_index.columns]\n",
    "OlderMen_index.head()\n",
    "#OlderMen = OlderMen_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for Older Women\n",
    "OlderWomen = gb.get_group('OlderWomen')[Col_Grps]\n",
    "#OlderWomen.head()\n",
    "OlderWomen_index = OlderWomen.set_index('UniqueID')\n",
    "OlderWomen_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.OlderWomen').replace('High_Confidence_Limit','High_CL.OlderWomen').replace('Data_Value','Data_Value.OlderWomen') \\\n",
    "                           for hdr in OlderWomen_index.columns]\n",
    "OlderWomen_index.head()\n",
    "#OlderWomen = OlderWomen_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pap Smears\n",
    "PapSmear = gb.get_group('PapSmear')[Col_Grps]\n",
    "#PapSmear.head()\n",
    "PapSmear_index = PapSmear.set_index('UniqueID')\n",
    "PapSmear_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.PapSmear').replace('High_Confidence_Limit','High_CL.PapSmear').replace('Data_Value','Data_Value.PapSmear') \\\n",
    "                           for hdr in PapSmear_index.columns]\n",
    "PapSmear_index.head()\n",
    "#PapSmear = PapSmear_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical Health is Bad\n",
    "PhysHlthBad = gb.get_group('PhysHlthBad')[Col_Grps]\n",
    "#PhysHlthBad.head()\n",
    "PhysHlthBad_index = PhysHlthBad.set_index('UniqueID')\n",
    "PhysHlthBad_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.PhysHlthBad').replace('High_Confidence_Limit','High_CL.PhysHlthBad').replace('Data_Value','Data_Value.PhysHlthBad') \\\n",
    "                           for hdr in PhysHlthBad_index.columns]\n",
    "PhysHlthBad_index.head()\n",
    "#PhysHlthBad = PhysHlthBad_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep litle (less than 7 hrs, day)\n",
    "SleepLittle = gb.get_group('SleepLittle')[Col_Grps]\n",
    "#SleepLittle.head()\n",
    "SleepLittle_index = SleepLittle.set_index('UniqueID')\n",
    "SleepLittle_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.SleepLittle').replace('High_Confidence_Limit','High_CL.SleepLittle').replace('Data_Value','Data_Value.SleepLittle') \\\n",
    "                           for hdr in SleepLittle_index.columns]\n",
    "SleepLittle_index.head()\n",
    "#SleepLittle = SleepLittle_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroke\n",
    "Stroke = gb.get_group('Stroke')[Col_Grps]\n",
    "#Stroke.head()\n",
    "Stroke_index = Stroke.set_index('UniqueID')\n",
    "Stroke_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.Stroke').replace('High_Confidence_Limit','High_CL.Stroke').replace('Data_Value','Data_Value.Stroke') \\\n",
    "                           for hdr in Stroke_index.columns]\n",
    "Stroke_index.head()\n",
    "#Stroke = Stroke_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population with Missing Teeth\n",
    "TeethLost = gb.get_group('TeethLost')[Col_Grps]\n",
    "#TeethLost.head()\n",
    "TeethLost_index = TeethLost.set_index('UniqueID')\n",
    "TeethLost_index.columns = [hdr.replace('Low_Confidence_Limit','Low_CL.TeethLost').replace('High_Confidence_Limit','High_CL.TeethLost').replace('Data_Value','Data_Value.TeethLost') \\\n",
    "                           for hdr in TeethLost_index.columns]\n",
    "TeethLost_index.head()\n",
    "#TeethLost = TeethLost_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://towardsdatascience.com/lets-make-a-map-using-geopandas-pandas-and-matplotlib-to-make-a-chloropleth-map-dddc31c1983d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time to merge all of the datasets together into one big dataset (but I know it's going to be a lot of intermediates!)\n",
    "# There might be an easier way of doing this in Python, but I'm out of practice enough not to think about it.\n",
    "# Also, really jetlagged. :D\n",
    "\n",
    "#'Arthritis_index', 'Asthma_index', 'BngDrnk_index', 'CHD_index', 'COPD_index', 'Cancer_index', 'Cholesterol_index', 'CurrSmoke_index', 'DentalVisits_index', 'Diabetes_index', 'DocVisits_index', 'FecBldTst_index', 'HighBP_index', 'HighChol_index', 'HtnMeds_index', 'KidneyDis_index', 'Mammo_index', 'MentHlth_index', 'NoHlthIns_index', 'NoPhysAct_index', 'Obesity_index', 'OlderMen_index', 'OlderWomen_index', 'PapSmear_index', 'PhysHlthBad_index', 'SleepLittle_index', 'Stroke_index', 'TeethLost_index' \n",
    "\n",
    "int_table1 = pd.merge(Arthritis_index, Asthma_index, on='UniqueID', how='outer')\n",
    "#int_table1\n",
    "\n",
    "int_table2 = pd.merge(int_table1, BngDrnk_index, on='UniqueID', how='outer')\n",
    "#int_table2\n",
    "\n",
    "del int_table1 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table3 = pd.merge(int_table2, CHD_index, on='UniqueID', how='outer')\n",
    "#int_table3\n",
    "\n",
    "del int_table2 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table4 = pd.merge(int_table3, COPD_index, on='UniqueID', how='outer')\n",
    "#int_table4\n",
    "\n",
    "del int_table3 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table5 = pd.merge(int_table4, Cancer_index, on='UniqueID', how='outer')\n",
    "#int_table5\n",
    "\n",
    "del int_table4 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table6 = pd.merge(int_table5, Cholesterol_index, on='UniqueID', how='outer')\n",
    "#int_table6\n",
    "\n",
    "del int_table5 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table7 = pd.merge(int_table6, CurrSmoke_index, on='UniqueID', how='outer')\n",
    "#int_table7\n",
    "\n",
    "del int_table6 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table8 = pd.merge(int_table7, DentalVisits_index, on='UniqueID', how='outer')\n",
    "#int_table8\n",
    "\n",
    "del int_table7 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table9 = pd.merge(int_table8, Diabetes_index, on='UniqueID', how='outer')\n",
    "#int_table9\n",
    "\n",
    "del int_table8 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table10 = pd.merge(int_table9, DocVisits_index, on='UniqueID', how='outer')\n",
    "#int_table10\n",
    "\n",
    "del int_table9 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table11 = pd.merge(int_table10, FecBldTst_index, on='UniqueID', how='outer')\n",
    "#int_table11\n",
    "\n",
    "del int_table10 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table12 = pd.merge(int_table11, HighBP_index, on='UniqueID', how='outer')\n",
    "#int_table12\n",
    "\n",
    "del int_table11 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table13 = pd.merge(int_table12, HighChol_index, on='UniqueID', how='outer')\n",
    "#int_table13\n",
    "\n",
    "del int_table12 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table14 = pd.merge(int_table13, HtnMeds_index, on='UniqueID', how='outer')\n",
    "#int_table14\n",
    "\n",
    "del int_table13 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table15 = pd.merge(int_table14, KidneyDis_index, on='UniqueID', how='outer')\n",
    "#int_table15\n",
    "\n",
    "del int_table14 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table16 = pd.merge(int_table15, Mammo_index, on='UniqueID', how='outer')\n",
    "#int_table16\n",
    "\n",
    "del int_table15 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table17 = pd.merge(int_table16, MentHlth_index, on='UniqueID', how='outer')\n",
    "#int_table17\n",
    "\n",
    "del int_table16 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table18 = pd.merge(int_table17, NoHlthIns_index, on='UniqueID', how='outer')\n",
    "#int_table18\n",
    "\n",
    "del int_table17 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table19 = pd.merge(int_table18, NoPhysAct_index, on='UniqueID', how='outer')\n",
    "#int_table19\n",
    "\n",
    "del int_table18 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table20 = pd.merge(int_table19, Obesity_index, on='UniqueID', how='outer')\n",
    "#int_table20\n",
    "\n",
    "del int_table19 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table21 = pd.merge(int_table20, OlderMen_index, on='UniqueID', how='outer')\n",
    "#int_table21\n",
    "\n",
    "del int_table20 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table22 = pd.merge(int_table21, OlderWomen_index, on='UniqueID', how='outer')\n",
    "#int_table22\n",
    "\n",
    "del int_table21 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table23 = pd.merge(int_table22, PapSmear_index, on='UniqueID', how='outer')\n",
    "#int_table23\n",
    "\n",
    "del int_table22 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table24 = pd.merge(int_table23, PhysHlthBad_index, on='UniqueID', how='outer')\n",
    "#int_table24\n",
    "\n",
    "del int_table23 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table25 = pd.merge(int_table24, SleepLittle_index, on='UniqueID', how='outer')\n",
    "#int_table25\n",
    "\n",
    "del int_table24 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table26 = pd.merge(int_table25, Stroke_index, on='UniqueID', how='outer')\n",
    "#int_table26\n",
    "\n",
    "del int_table25 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "int_table27 = pd.merge(int_table26, TeethLost_index, on='UniqueID', how='outer')\n",
    "#int_table26\n",
    "\n",
    "del int_table26 #deleting previous intermediary as I go to save memory\n",
    "\n",
    "## FINAL TABLE\n",
    "int_table27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAMING DATASET - Resetting index and reordering variables\n",
    "\n",
    "df_Ind_int = pd.DataFrame(int_table27.to_records())\n",
    "df_Ind_int_index = df_Ind_int.set_index('UniqueID')\n",
    "#df_Indicators.drop(columns=['Measure', 'Measure_Short'])\n",
    "\n",
    "df_Indicators = df_Ind_int_index.drop(['Measure', 'Measure_Short'], axis=1)\n",
    "df_Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Indicators.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder tables for nicer appearance\n",
    "\n",
    "indicator_table = df_Indicators[['DataValueTypeID','CityName','StateAbbr','CityFIPS','TractFIPS','PopCount','GeoLocation',\n",
    "                                     'Data_Value.Arthritis','Low_CL.Arthritis','High_CL.Arthritis',\n",
    "                                     'Data_Value.Asthma','Low_CL.Asthma','High_CL.Asthma',\n",
    "                                     'Data_Value.BngDrnk','Low_CL.BngDrnk','High_CL.BngDrnk',\n",
    "                                     'Data_Value.CHD','Low_CL.CHD','High_CL.CHD',\n",
    "                                     'Data_Value.COPD','Low_CL.COPD','High_CL.COPD',\n",
    "                                     'Data_Value.Cancer','Low_CL.Cancer','High_CL.Cancer',\n",
    "                                     'Data_Value.Cholesterol','Low_CL.Cholesterol','High_CL.Cholesterol',\n",
    "                                     'Data_Value.CurrSmoke','Low_CL.CurrSmoke','High_CL.CurrSmoke',\n",
    "                                     'Data_Value.DentalVisits','Low_CL.DentalVisits','High_CL.DentalVisits',\n",
    "                                     'Data_Value.Diabetes','Low_CL.Diabetes','High_CL.Diabetes',\n",
    "                                     'Data_Value.DocVisits','Low_CL.DocVisits','High_CL.DocVisits',\n",
    "                                     'Data_Value.FecBldTst','Low_CL.FecBldTst','High_CL.FecBldTst',\n",
    "                                     'Data_Value.HighBP','Low_CL.HighBP','High_CL.HighBP',\n",
    "                                     'Data_Value.HighChol','Low_CL.HighChol','High_CL.HighChol',\n",
    "                                     'Data_Value.HtnMeds','Low_CL.HtnMeds','High_CL.HtnMeds',\n",
    "                                     'Data_Value.KidneyDis','Low_CL.KidneyDis','High_CL.KidneyDis',\n",
    "                                     'Data_Value.Mammo','Low_CL.Mammo','High_CL.Mammo',\n",
    "                                     'Data_Value.MentHlth', 'Low_CL.MentHlth','High_CL.MentHlth',\n",
    "                                     'Data_Value.NoHlthIns','Low_CL.NoHlthIns','High_CL.NoHlthIns',\n",
    "                                     'Data_Value.NoPhysAct','Low_CL.NoPhysAct','High_CL.NoPhysAct',\n",
    "                                     'Data_Value.Obesity','Low_CL.Obesity','High_CL.Obesity',\n",
    "                                     'Data_Value.OlderMen','Low_CL.OlderMen','High_CL.OlderMen',\n",
    "                                     'Data_Value.OlderWomen','Low_CL.OlderWomen','High_CL.OlderWomen',\n",
    "                                     'Data_Value.PapSmear','Low_CL.PapSmear','High_CL.PapSmear',\n",
    "                                     'Data_Value.PhysHlthBad','Low_CL.PhysHlthBad','High_CL.PhysHlthBad',\n",
    "                                     'Data_Value.SleepLittle','Low_CL.SleepLittle','High_CL.SleepLittle',\n",
    "                                     'Data_Value.Stroke','Low_CL.Stroke','High_CL.Stroke',\n",
    "                                     'Data_Value.TeethLost','Low_CL.TeethLost','High_CL.TeethLost']]\n",
    "\n",
    "indicator_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe all of the seperate indicators for each of the variables\n",
    "indicator_description = indicator_table.dropna().describe()\n",
    "indicator_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_description.to_csv('C:/Users/mmsch/OneDrive/Desktop/Data Science/indicator_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPORTING OUT MY LAST DATAFRAME INTO A CSV FILE FOR USE IN GEOPANDAS (which is a local install on my computer)\n",
    "\n",
    "indicator_table.to_csv('C:/Users/mmsch/OneDrive/Desktop/Data Science/indicator_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note - I am linking Geopandas code onto Github and simply exporting out my dataset as a CSV. (03/03/2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %load https://raw.githubusercontent.com/censusreporter/census-shapefile-utils/master/fetch_shapefiles.py\n",
    "# '''\n",
    "# This script will download TIGER data shapefiles from the Census FTP site.\n",
    "# It can be used to download a set of geographies defined in GEO_TYPES_LIST,\n",
    "# or can be used to fetch files for a single state and/or single geography type.\n",
    "# Pass an -s argument to limit by state, pass a -g argument to limit\n",
    "# to a single geography type, and/or pass a -y argument to change the year\n",
    "# from 2012 to something else (e.g. 2015).\n",
    "\n",
    "#     >> python fetch_shapefiles.py\n",
    "#     >> python fetch_shapefiles.py -s WA\n",
    "#     >> python fetch_shapefiles.py -g place\n",
    "#     >> python fetch_shapefiles.py -y 2015\n",
    "#     >> python fetch_shapefiles.py -s WA -g place -y 2015\n",
    "\n",
    "# If you use the -s argument to fetch files for a single state, the script\n",
    "# will also download the national county, state and congressional district\n",
    "# files that include data for your chosen state.\n",
    "\n",
    "# The script will create DOWNLOAD_DIR and EXTRACT_DIR directories\n",
    "# if necessary, fetch a zipfile or set of zipfiles from the Census website,\n",
    "# then extract the shapefiles from each zipfile retrieved.\n",
    "\n",
    "# DISABLE_AUTO_DOWNLOADS will prevent certain geography types from being\n",
    "# automatically downloaded if no -g argument is passed to fetch_shapefiles.py.\n",
    "# This may be useful because certain files, such as those for Zip Code\n",
    "# Tabulation Areas, are extremely large. You can still target any geography\n",
    "# in GEO_TYPES_LIST specifically, however. So to fetch the ZCTA data:\n",
    "\n",
    "#     >> python fetch_shapefiles.py -g zcta5\n",
    "# '''\n",
    "\n",
    "# import optparse\n",
    "# import os\n",
    "# import sys\n",
    "# import zipfile\n",
    "# from os.path import isdir, join, normpath\n",
    "\n",
    "# try:\n",
    "#     from six.moves.urllib import request as urllib2\n",
    "# except ImportError:\n",
    "#     import urllib2\n",
    "\n",
    "# from __init__ import (DOWNLOAD_DIR, EXTRACT_DIR, STATE_ABBREV_LIST,\n",
    "#                       GEO_TYPES_LIST, DISABLE_AUTO_DOWNLOADS,\n",
    "#                       get_fips_code_for_state)\n",
    "\n",
    "# FTP_HOME = 'ftp://ftp2.census.gov/geo/tiger/TIGER2012/'\n",
    "\n",
    "\n",
    "# def get_filename_list_from_ftp(target, state):\n",
    "#     target_files = urllib2.urlopen(target).read().splitlines()\n",
    "#     filename_list = []\n",
    "\n",
    "#     for line in target_files:\n",
    "#         filename = '%s%s' % (target, line.decode().split()[-1])\n",
    "#         filename_list.append(filename)\n",
    "\n",
    "#     if state:\n",
    "#         state_check = '_%s_' % get_fips_code_for_state(state)\n",
    "#         filename_list = filter(\n",
    "#             lambda filename:\n",
    "#                 state_check in filename or\n",
    "#                 ('_us_' in filename and\n",
    "#                  '_us_zcta5' not in filename),\n",
    "#             filename_list\n",
    "#         )\n",
    "\n",
    "#     return filename_list\n",
    "\n",
    "\n",
    "# def get_content_length(u):\n",
    "#     # u is returned by urllib2.urlopen\n",
    "#     if sys.version_info[0] == 2:\n",
    "#         return int(u.info().getheader(\"Content-Length\"))\n",
    "#     else:\n",
    "#         return int(u.headers[\"Content-Length\"])\n",
    "\n",
    "\n",
    "# def download_files_in_list(filename_list, force=False):\n",
    "#     downloaded_filename_list = []\n",
    "#     for file_location in filename_list:\n",
    "#         filename = '%s/%s' % (DOWNLOAD_DIR, file_location.split('/')[-1])\n",
    "#         if force or not os.path.exists(filename):\n",
    "#             # Only download if required.\n",
    "#             u = urllib2.urlopen(file_location)\n",
    "#             f = open(filename, 'wb')\n",
    "#             file_size = get_content_length(u)\n",
    "\n",
    "#             print(\"Downloading: %s Bytes: %s\" % (filename, file_size))\n",
    "#             file_size_dl = 0\n",
    "#             block_sz = 8192\n",
    "#             while True:\n",
    "#                 buffer = u.read(block_sz)\n",
    "#                 if not buffer:\n",
    "#                     break\n",
    "\n",
    "#                 file_size_dl += len(buffer)\n",
    "#                 f.write(buffer)\n",
    "#                 status = r\"%10d  [%3.2f%%]\" % (\n",
    "#                     file_size_dl, file_size_dl * 100. / file_size)\n",
    "#                 status = status + chr(8) * (len(status) + 1)\n",
    "#                 sys.stdout.write(status)\n",
    "#                 sys.stdout.flush()\n",
    "\n",
    "#             f.close()\n",
    "#         downloaded_filename_list.append(filename)\n",
    "\n",
    "#     return downloaded_filename_list\n",
    "\n",
    "\n",
    "# def extract_downloaded_file(filename, remove_on_error=True):\n",
    "#     zip_dir = filename.replace('.zip', '').split('/')[-1]\n",
    "#     target_dir = normpath(join(EXTRACT_DIR, zip_dir))\n",
    "\n",
    "#     print(\"Extracting: \" + filename + \" ...\")\n",
    "#     try:\n",
    "#         zipped = zipfile.ZipFile(filename, 'r')\n",
    "#     except zipfile.BadZipFile as ze:\n",
    "#         if remove_on_error:\n",
    "#             os.remove(filename)\n",
    "#             raise Exception(\n",
    "#                 \"Removed corrupt zip file (%s). Retry download.\" % filename)\n",
    "#         raise ze\n",
    "\n",
    "#     zipped.extractall(target_dir)\n",
    "#     zipped.close()\n",
    "\n",
    "\n",
    "# def get_one_geo_type(geo_type, state=None, year='2012'):\n",
    "#     target = '%s%s/' % (FTP_HOME.replace('2012', year), geo_type.upper())\n",
    "\n",
    "#     print(\"Finding files in: \" + target + \" ...\")\n",
    "#     filename_list = get_filename_list_from_ftp(target, state)\n",
    "#     downloaded_filename_list = download_files_in_list(filename_list)\n",
    "\n",
    "#     for filename in downloaded_filename_list:\n",
    "#         extract_downloaded_file(filename)\n",
    "\n",
    "\n",
    "# def get_all_geo_types(state=None, year='2012'):\n",
    "#     AUTO_DOWNLOADS = filter(\n",
    "#         lambda geo_type: geo_type not in DISABLE_AUTO_DOWNLOADS,\n",
    "#         GEO_TYPES_LIST\n",
    "#     )\n",
    "#     for geo_type in AUTO_DOWNLOADS:\n",
    "#         get_one_geo_type(geo_type, state, year)\n",
    "\n",
    "\n",
    "# def process_options(arglist=None):\n",
    "#     global options, args\n",
    "#     parser = optparse.OptionParser()\n",
    "#     parser.add_option(\n",
    "#         '-s', '--state',\n",
    "#         dest='state',\n",
    "#         help='specific state to download',\n",
    "#         choices=STATE_ABBREV_LIST,\n",
    "#         default=None\n",
    "#     )\n",
    "#     parser.add_option(\n",
    "#         '-g', '--geo', '--geo_type',\n",
    "#         dest='geo_type',\n",
    "#         help='specific geographic type to download',\n",
    "#         choices=GEO_TYPES_LIST,\n",
    "#         default=None\n",
    "#     )\n",
    "#     parser.add_option(\n",
    "#         '-y', '--year',\n",
    "#         dest='year',\n",
    "#         help='specific year to download',\n",
    "#         default='2012'\n",
    "#     )\n",
    "\n",
    "#     options, args = parser.parse_args(arglist)\n",
    "#     return options, args\n",
    "\n",
    "\n",
    "# def main(args=None):\n",
    "#     \"\"\"\n",
    "#     >> python fetch_shapefiles.py\n",
    "#     >> python fetch_shapefiles.py -s WA\n",
    "#     >> python fetch_shapefiles.py -g place\n",
    "#     >> python fetch_shapefiles.py -s WA -g place\n",
    "#     \"\"\"\n",
    "#     if args is None:\n",
    "#         args = sys.argv[1:]\n",
    "#     options, args = process_options(args)\n",
    "\n",
    "#     # make sure we have the expected directories\n",
    "#     for path in [DOWNLOAD_DIR, EXTRACT_DIR]:\n",
    "#         if not isdir(path):\n",
    "#             os.makedirs(path)\n",
    "\n",
    "#     # get one geo_type or all geo_types\n",
    "#     if options.geo_type:\n",
    "#         get_one_geo_type(\n",
    "#             geo_type = options.geo_type,\n",
    "#             state = options.state,\n",
    "#             year=options.year\n",
    "#         )\n",
    "#     else:\n",
    "#         get_all_geo_types(\n",
    "#             state = options.state,\n",
    "#             year=options.year\n",
    "#         )\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - as of 03/03/2019, Geopandas hasn't been working on this notebook.\n",
    "# Until I properly debug this, I am commenting out all geopandas references, exporting my dataset\n",
    "# as a CSV, then linking the Python file onto Github - Michelle\n",
    "\n",
    "# # To date, my entire Jupyter analysis has been using shapefiles online. \n",
    "# # Until I figure out how to get the Census Reporter Python file working, I will change my CD to the Data Science\n",
    "# # subfolder set up on my Desktop.\n",
    "\n",
    "# cd = \"C:/Users/mmsch/OneDrive/Desktop/Data Science\"\n",
    "\n",
    "# # set the filepath and load in a shapefile\n",
    "# fp = \"C:/Users/mmsch/OneDrive/Desktop/Data Science/tl_2018_us_ttract.shp\"\n",
    "\n",
    "# map_df = gpd.read_file(fp)\n",
    "# # check data type so we can see that this is not a normal dataframe, but a GEOdataframe\n",
    "# map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other interesting links!\n",
    "\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/04.13-geographic-data-with-basemap.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
